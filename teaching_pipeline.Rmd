---
title: "Teaching Pipeline"
author: "Lukas Dreyling & Henrique Valim"
date: "2022-09-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r klippy, echo=FALSE, include=TRUE}
library(klippy)
klippy::klippy()
```

## Program List 

 * guppy (will run internally?)
    + already demultiplexes and filters low quality reads
 * Decona pipeline
    + CD-Hit &rarr; clustering Reads 
    + Minimap2 &rarr; align reads
    + Racon &rarr; make consensus sequences
    + Medaka &rarr; plosih the sequences
 * Blast+
 * R (and RStudio)
    + packages: vegan, microbiome, ggplot, here 
    
## Basecalling and Demultiplexing 

### Guppy 

```{bash, eval = F}
#!/bin/bash

#SBATCH -n 4
#SBATCH -p gpu
#SBATCH -t 12:00:00
#SBATCH -J guppy_gpu
#SBATCH -o guppy_gpu.o%j
#SBATCH -e guppy_gpu.e%j

module load guppy

cd $SLURM_SUBMIT_DIR

guppy_basecaller -i fast5_pass -s fastq_9.4_pass -c dna_r9.4.1_450bps_hac.cfg -x "cuda:1" --barcode_kits SQK-RBK001
guppy_basecaller -i fast5_skip -s fastq_9.4_skip -c dna_r9.4.1_450bps_hac.cfg -x "cuda:1" --barcode_kits SQK-RBK001

scontrol show job $SLURM_JOB_ID

```

- barcode removal is on by default if using the demultiplexing flag (--barcode_kits)

### Quality and trimming with Nanofilt  

```{bash}
# Move into the directory where the fastq files that passed the initial quality control are stored 
cd fastq_pass 

# Unzip the files 
gzip *.gz

# Make one big fastq file for processing 
cat *.fastq > ../full_sequences.fastq

# Make a big fastq file and filter it based on Quality score, minimum and maximum length
cat *.fastq | NanoFilt -q 10 -l 500 --maxlength 2000 > ../full_sequences_filtered.fastq

cd ..
```


 * later we need the reads in fasta format so we need to convert 
```{bash}
# Make a directory to store the fasta files in 
mkdir fasta_pass

# Turn the two .fastq files of the full and filtered sequences into .fasta files
for i in *.fastq ;  do
            if [ -e "$i" ] ; then
            cat "$i" | grep -A 1 'runid' | sed '/^--$/d' | sed 's/^@/>/' | awk '{print $1}' > "${i%%.*}.fasta" ; 
            fi
            done
            
# Compare how many sequences we filtered out
grep -c '^>' *.fasta | less

# Move the .fasta files to the folder we created before.
mv ./*.fasta ./fasta_pass

```

## Clustering the Reads

 * to obtain groups of similar reads 
 * potentially the same species/OTU
 * CD-Hit
 * combine all reads from one barcode in one big fasta to get accurate clusters
 
```{bash}
# Enter the folder where we stored the .fasta files
cd fasta_pass

# Activate Conda environment where all the programms are installed 
conda activate master_class

# Sub sample sequences so we can run all commands locally 
seqtk sample -s100 full_sequences_filtered.fasta 10000 > subset_sequences.fa

# Clustering reads to make a list of most abundant, representative reads
    for fasta in *.fa ; do
    if [ -e "$fasta" ] ; then
    # This is the actual clustering command
    cd-hit-est -i "$fasta" -o cluster_representatives_${fasta%.*} -c 0.8 -n 5 -d 0 -M 0 -T 0 -g 1 > report_"${fasta%.*}.txt"; 
    fi
    done

# Have a look at the cluster sizes
for clstr in *.clstr ; do
    if [ -e "$clstr" ] ; then
    # Read distribution will be summarized in report_***.txt within the folder.
    plot_len1.pl "$clstr" \
    1,2-4,5-9,10-19,20-49,50-99,100-299,300-499,500-999,1000-10000 \
    >> size_report_"$clstr".txt ;
    fi
    done
    
    
# Create files with clusters of a certain size
 make_multi_seq.pl *.fa *.clstr multi-seq 10 

```

## Align clustered Reads

```{bash}
# Navigate to the folder with our cluster sequences
cd multi-seq

# Add .fa to all filenames, unless they are already .fa (so we do not get into an infinite loop)
for file in *; do 
case "$file" in *.fa) echo skipped $file;; *) mv "$file" "$file".fa; esac; done

# Align the reads of each cluster to the first sequence of that cluster that is the so-called  "parent"/"reference" sequence
    for file in *.fa ;  do
        if [ -e "${file}" ] ; then
        echo "Aligning and making draft assembly of $file...";
        # Extract 1st sequence of each file as a reference point
        tail -n 2 "${file}" > ref_"${file}"sta;
        # Aligning all data in the cluster to the reference sequence
        minimap2 -ax map-ont ref_"${file}"sta "${file}" -t 3 > align_"${file}".sam ;

        # Assemble the clustered sequences.
        # Racon settings optimized for Medaka: -m 8 -x -6 -g -8 -w 500
        racon -m 8 -x -6 -g -8 -w 500 -t 3 "${file}" align_"${file}".sam ref_"${file}"sta > polished_"${file}"sta ;
        fi
        done
        
cd .. 

mkdir polished_seqs 

mv ./multi-seq/*polished* ./polished_seqs
mv ./multi-seq/*.fa ./polished_seqs
```

## Polishing with Medaka

```{bash}
# Because Medaka does not run nicely in the previous conda environment we will need to move to another
cd polished_seqs

conda activate decona 

        for fa in *.fa ;  do
        if [ -e "polished_${fa}sta" ] ; then
        echo "polishing ${fa} Racon sequence with Medaka..."
        medaka_consensus -i "${fa}" -d "polished_${fa}sta" -o ../"consensus_medaka_${fa}" -t 10 ;
        fi
        done

    #Change names of Medaka consensus to have their cluster's name
    #move them one folder up
    for folders in consensus_medaka_*; do
    if [ -e "${folders}" ] ; then
    (
        cd "${folders}" || exit ;
        [ ! -f consensus.fasta ] || mv consensus.fasta "${folders}sta"
        [ ! -f "${folders}sta" ] || mv "${folders}sta" ..
    )
    fi
    done
```

## Blast search against UNITE database

```{bash}
# Make a searchable BLAST database out of the UNITE general .fasta release. 
makeblastdb -in sh_general_release_dynamic_27.10.2022.fasta -title unite_db -parse_seqids -dbtype nucl

# Match the consensus sequences to the UNITE database 
blastn -query consensus.fasta \
-strand both \
-task megablast \
-db unite_db \
-out fungal_taxonomy.txt \
-outfmt 6 \
-num_threads 3 \
-max_target_seqs 1 \
-perc_identity 0.9
```

