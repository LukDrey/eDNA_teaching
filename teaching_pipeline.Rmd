---
title: "Teaching Pipeline"
author: "Lukas Dreyling & Henrique Valim"
date: "2022-09-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Program List 

 * guppy (run during sequencing)
    + already demultiplexes and filters low quality reads
 * Decona pipeline
    + CD-Hit &rarr; clustering Reads 
    + Minimap2 &rarr; align reads
    + Racon &rarr; make consensus sequences
    + Medaka &rarr; plosih the sequences
 * Blast+
 * R (and RStudio)
    
## Basecalling and Demultiplexing 

### Guppy 

```{bash, eval = F}
#!/bin/bash

#SBATCH -n 4
#SBATCH -p gpu
#SBATCH -t 12:00:00
#SBATCH -J guppy_gpu
#SBATCH -o guppy_gpu.o%j
#SBATCH -e guppy_gpu.e%j

module load guppy

cd $SLURM_SUBMIT_DIR

guppy_basecaller -i fast5_pass -s fastq_9.4_pass -c dna_r9.4.1_450bps_hac.cfg -x "cuda:1" --barcode_kits SQK-RBK001
guppy_basecaller -i fast5_skip -s fastq_9.4_skip -c dna_r9.4.1_450bps_hac.cfg -x "cuda:1" --barcode_kits SQK-RBK001

scontrol show job $SLURM_JOB_ID

```

- barcode removal is on by default if using the demultiplexing flag (--barcode_kits)

### Quality and trimming with Nanofilt  

```{bash, eval = F}
# Move into the directory where the fastq files that passed the initial quality control are stored 
cd fastq_pass 

# Unzip the files 
gzip *.gz

# Make one big fastq file for processing 
cat *.fastq > ../full_sequences.fastq

# Make a big fastq file and filter it based on Quality score, minimum and maximum length
cat *.fastq | NanoFilt -q 10 -l 500 --maxlength 2000 > ../full_sequences_filtered.fastq

cd ..
```


 * later we need the reads in fasta format so we need to convert 
```{bash, eval = F}
# Make a directory to store the fasta files in 
mkdir fasta_pass

# Turn the two .fastq files of the full and filtered sequences into .fasta files
for i in *.fastq ;  do
            if [ -e "$i" ] ; then
            cat "$i" | grep -A 1 'runid' | sed '/^--$/d' | sed 's/^@/>/' | awk '{print $1}' > "${i%%.*}.fasta" ; 
            fi
            done
            
# Compare how many sequences we filtered out
grep -c '^>' *.fasta | less

# Move the .fasta files to the folder we created before.
mv ./*.fasta ./fasta_pass

```

## Clustering the Reads

 * to obtain groups of similar reads 
 * potentially the same species/OTU
 * CD-Hit
 * combine all reads from one barcode in one big fasta to get accurate clusters
 
```{bash, eval = F}
# Enter the folder where we stored the .fasta files
cd fasta_pass

# Activate Conda environment where all the programms are installed 
conda activate master_class

# Sub sample sequences so we can run all commands locally 
seqtk sample -s100 full_sequences_filtered.fasta 10000 > subset_sequences.fa

# Clustering reads to make a list of most abundant, representative reads
    for fasta in *.fa ; do
    if [ -e "$fasta" ] ; then
    # This is the actual clustering command
    cd-hit-est -i "$fasta" -o cluster_representatives_${fasta%.*} -c 0.8 -n 5 -d 0 -M 0 -T 0 -g 1 > report_"${fasta%.*}.txt"; 
    fi
    done

# Have a look at the cluster sizes
for clstr in *.clstr ; do
    if [ -e "$clstr" ] ; then
    # Read distribution will be summarized in report_***.txt within the folder.
    plot_len1.pl "$clstr" \
    1,2-4,5-9,10-19,20-49,50-99,100-299,300-499,500-999,1000-10000 \
    >> size_report_"$clstr".txt ;
    fi
    done
    
    
# Create files with clusters of a certain size
 make_multi_seq.pl *.fa *.clstr multi-seq 10 

```

## Align clustered Reads

```{bash, eval = F}
# Navigate to the folder with our cluster sequences
cd multi-seq

# Add .fa to all filenames, unless they are already .fa (so we do not get into an infinite loop)
for file in *; do 
case "$file" in *.fa) echo skipped $file;; *) mv "$file" "$file".fa; esac; done

# Align the reads of each cluster to the first sequence of that cluster that is the so-called  "parent"/"reference" sequence
    for file in *.fa ;  do
        if [ -e "${file}" ] ; then
        echo "Aligning and making draft assembly of $file...";
        # Extract 1st sequence of each file as a reference point
        tail -n 2 "${file}" > ref_"${file}"sta;
        # Aligning all data in the cluster to the reference sequence
        minimap2 -ax map-ont ref_"${file}"sta "${file}" -t 3 > align_"${file}".sam ;

        # Assemble the clustered sequences.
        # Racon settings optimized for Medaka: -m 8 -x -6 -g -8 -w 500
        racon -m 8 -x -6 -g -8 -w 500 -t 3 "${file}" align_"${file}".sam ref_"${file}"sta > polished_"${file}"sta ;
        fi
        done
        
cd .. 

mkdir polished_seqs 

mv ./multi-seq/*polished* ./polished_seqs
mv ./multi-seq/*.fa ./polished_seqs
```

## Polishing with Medaka

```{bash, eval = F}
# Because Medaka does not run nicely in the previous conda environment we will need to move to another
cd polished_seqs

conda activate decona 

        for fa in *.fa ;  do
        if [ -e "polished_${fa}sta" ] ; then
        echo "polishing ${fa} Racon sequence with Medaka..."
        medaka_consensus -i "${fa}" -d "polished_${fa}sta" -o ./"consensus_medaka_${fa}" -t 10 ;
        fi
        done

    #Change names of Medaka consensus to have their cluster's name
    #move them one folder up
    for folders in consensus_medaka_*; do
    if [ -e "${folders}" ] ; then
    (
        cd "${folders}" || exit ;
        [ ! -f consensus.fasta ] || mv consensus.fasta "${folders}sta"
        [ ! -f "${folders}sta" ] || mv "${folders}sta" ../../final_seqs
    )
    fi
    done

cd .. 

# Make one big fasta file for processing 
cat *.fasta > ./full_consensus.fasta

# Change the names of the Sequences to more manageable names. 
sed 's/^>.*$/>asv/' full_consensus.fasta | perl -pe 's/asv/$i++/ge' | sed 's/^>/>asv/' > full_consensus.fasta
```

## Move to R and taxonomy assignment 

 * dada2 naive bayesian classifier 
 * matching taxonomy against the UNITE database &rarr; special curated database of fungal ITS sequences
```{R, eval = F}
# Load the packages we need for the analysis. 
library(dada2)
library(tidyverse)
library(Biostrings)

# Set the paths to where the files are stored. 
unite.ref <- '/home/evo9-schmitt/Documents/Data/sh_general_release_dynamic_27.10.2022.fasta'
sequences <- '/home/evo9-schmitt/Documents/Data/full_consensus_rename.fasta'

# Taxonomy assignment with dada2 (https://benjjneb.github.io/dada2/index.html). 
fungi_taxa <- dada2::assignTaxonomy(sequences, unite.ref, multithread = TRUE, tryRC = TRUE, minBoot = 80)

# Convert the dada2 output to a dataframe 
tax_fungi <- base::as.data.frame(fungi_taxa) %>%
  tibble::rownames_to_column('sequence')
tax_fungi <- tax_fungi %>%
  dplyr::rename(sequence_fungi = sequence)

# Load the fungal reads from the fasta file with the consensus sequences. 
fungi_seqs_fasta <- Biostrings::readDNAStringSet('full_consensus.fasta')

# Make a dataframe of the sequences and their ASV ID. 
seq_name_fungi <- base::names(fungi_seqs_fasta)
sequence_fungi <- base::paste(fungi_seqs_fasta)
fungi_rep_seqs <- base::data.frame(seq_name_fungi, sequence_fungi)

# Join the taxonomy table and the representative sequences.
tax_clean_fungi <- dplyr::left_join(tax_fungi, fungi_rep_seqs, by = 'sequence_fungi')

# Split the taxonomy into different columns of taxonomic levels.
fungi_tax_fin <- tidyr::separate(tax_clean_fungi, Kingdom, c(NA, 'Kingdom') , sep = '__') %>%
  tidyr::separate(fungi_tax_fin, Phylum, c(NA, 'Phylum') , sep = '__') %>% 
  tidyr::separate(fungi_tax_fin, Class, c(NA, 'Class') , sep = '__') %>% 
  tidyr::separate(fungi_tax_fin, Order, c(NA, 'Order') , sep = '__') %>% 
  tidyr::separate(fungi_tax_fin, Family, c(NA, 'Family') , sep = '__') %>% 
  tidyr::separate(fungi_tax_fin, Genus, c(NA, 'Genus') , sep = '__') %>% 
  tidyr::separate(fungi_tax_fin, Species, c(NA, 'Species') , sep = '__')

# Rename the ASV_ID column. 
fungi_tax_fin <- dplyr::rename(fungi_tax_fin, ASV_ID = seq_name_fungi)

# Set rownames.
base::row.names(fungi_tax_fin) <- fungi_tax_fin$ASV_ID

# Remove the sequences and ASV IDs 
fungi_tax_fin$sequence_fungi <- NULL
fungi_tax_fin$ASV_ID <- NULL
```
